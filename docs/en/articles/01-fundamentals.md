# Are Data Contracts the key enabler of Decentralized Data Management?

It's 3 AM. The support team receives a critical alert: the data pipeline feeding the real-time sales dashboard is down. Preliminary analysis reveals that the e-commerce team modified the order data format without notice. A required field has been renamed, and now the entire processing chain is paralyzed. This situation, unfortunately too common, illustrates an often-neglected reality: data is not just an asset, it's a product that requires rigorous lifecycle management.

## Daily Life Without Data Contracts

Imagine a growing e-commerce company. Several teams work in parallel on different parts of the system:

- The e-commerce team manages the sales platform and generates transaction data
- The data science team develops recommendation models
- The BI team produces reports for management
- The marketing team uses customer data for campaigns

On the surface, everything works. But beneath, it's chaos:

- Data engineers spend their days fixing broken pipelines because a field changed type or name
- Data scientists discover their models are producing erroneous results due to silent changes in input data
- The BI team must constantly verify if metrics are still calculated the same way

Meetings are filled with questions like: "Who changed this field?", "Why is the data different today?", "How are we supposed to use this column?"

## The Hidden Cost of Missing Contracts

This situation has a real cost, often underestimated:

- Business decisions made on incorrect data
- Hours lost in debugging and reconciliation
- Delayed data projects
- Loss of trust in data
- Stress and frustration in teams

This situation becomes even more critical in a Data Mesh context, where data responsibility is decentralized to business domains. Take the example of a bank I recently assisted in its Data Mesh transformation. Each domain - credit, savings, insurance - became responsible for its own data. When data products give a single method to produce data, data contracts ensure consistency and define clear expectations between data producers and consumers.

On average, teams spend 40% of their time managing these coordination and quality issues. It's like building a house where each craftsman would use their own units of measurement, but at the scale of an entire city.

## The Emergence of Data Contracts

The Data Mesh transformation represents a fundamental shift in how organizations handle data, mirroring the evolution of software architecture in the API economy. In this model, each business domain owns and exposes its data products, much like microservices in a service-oriented architecture. Whether itâ€™s credit, savings, or insurance data in banking, or sales, logistics, and marketing data in retail, this decentralization enhances agility and aligns data closer to business needs.

However, just as uncoordinated API development leads to integration headaches, unmanaged data decentralization creates fragmentation. Without clear contracts, teams struggle with inconsistencies, spending up to 40% of their time resolving data quality issues, similarly of how teams dealt with poorly documented or unstable APIs. Data Contracts serve as a solution to these challenges, acting as the equivalent of API contracts: they define structure, enforce validation, and establish guarantees between producers and consumers, ensuring reliable and scalable data interactions across the organization.

Let's examine the typical architecture of a Data Contracts implementation:

```mermaid
graph TD
    A[Producers] -->|Publish| B[Data Contracts]
    B -->|Validated by| C[Validation Service]
    B -->|Stored in| D[Contract Registry]
    E[Consumers] -->|Discover| B
    F[CI/CD] -->|Automates| B
    G[Monitoring] -->|Monitors| B
    H[Governance] -->|Governs| B
    
    subgraph "Infrastructure"
        D
        C
    end
    
    subgraph "Processes"
        F
        G
        H
    end
```

This architecture illustrates the essential components of a Data Contracts system. The contract registry centralizes definitions, while the validation service ensures their compliance. Integration with CI/CD processes enables automation, while monitoring ensures continuous quality. Governance, finally, provides the framework necessary for controlled evolution.

## Open Data Contract Standard (ODCS)

Facing these challenges, a standard has emerged: the Open Data Contract Standard (ODCS). This isn't just another technical specification - it's a common language that allows teams to clearly communicate their expectations and commitments regarding data. Here's a concrete example of an ODCS contract for a customer data stream:

```yaml
openDataContract: "1.0.0"
info:
  title: "customer_profile"
  version: "1.0.0"
  domain: "customer"
  owner: 
    team: "customer-data"
    contact: "customer-data@company.com"

contracts:
  CustomerProfile:
    type: "batch"
    format: "parquet"
    schema:
      type: "struct"
      fields:
        - name: "customer_id"
          type: "string"
          description: "Unique customer identifier"
          constraints:
            - type: "not_null"
        - name: "email"
          type: "string"
          constraints:
            - type: "email_format"

quality:
  rules:
    - name: "email_validity"
      severity: "critical"
    - name: "recent_data"
      severity: "warning"

sla:
  freshness: "24h"
  availability: "99.9%"
```

Let's analyze each section of this contract in detail:

1. The contract header establishes its identity and governance. The `domain` field isn't just simple categorization - it explicitly ties this data to a responsible business unit. Contact information isn't administrative formality; it's a commitment of responsibility.

2. The `interface` section goes beyond simple technical description. The choice of Parquet format isn't arbitrary - it reflects a compromise between read performance and schema flexibility. Each schema field is documented and constrained, creating a clear framework for data quality.

3. Quality rules establish a clear hierarchy of potential problems. An invalid email address is considered critical as it can directly impact customer communication, while data freshness is a warning that deserves attention without necessarily triggering an alarm.

4. Operational SLAs aren't just goals - they represent a concrete service contract between the producer and its consumers. 24h freshness and 99.9% availability are measurable commitments that will guide architecture and operational choices.

## Implementation: From Concepts to Reality

Implementing data contracts in a datalake context is particularly relevant, especially in a medallion architecture (bronze, silver, gold). Let's take the example of the sales domain, where raw transaction data is progressively refined to feed critical analyses and dashboards.

The first contract established concerns the silver table of sales transactions. This table is a critical point: it cleans and standardizes raw data from the bronze layer and serves as a source of truth for creating gold layer aggregates.

```yaml
openDataContract: "1.0.0"
info:
  title: "sales_transactions_silver"
  version: "1.0.0"
  domain: "sales_analytics"
  owner: 
    team: "data-engineering"
    contact: "data-engineering@retail.com"

contracts:
  SalesTransaction:
    type: "batch"
    format: "delta"
    schema:
      fields:
        - name: "transaction_id"
          type: "string"
          description: "Unique transaction identifier"
          constraints:
            - type: "not_null"
            - type: "unique"
            
        - name: "transaction_date"
          type: "date"
          description: "Transaction date"
          constraints:
            - type: "not_null"
            - type: "not_future"
            
        - name: "store_id"
          type: "string"
          description: "Unique store identifier"
          constraints:
            - type: "not_null"
            - type: "reference"
              table: "dim_stores"
              field: "store_id"
            
        - name: "product_id"
          type: "string"
          description: "Unique product identifier"
          constraints:
            - type: "not_null"
            - type: "reference"
              table: "dim_products"
              field: "product_id"
              
        - name: "quantity"
          type: "integer"
          description: "Quantity sold"
          constraints:
            - type: "positive"
            
        - name: "unit_price"
          type: "decimal"
          description: "Unit price at time of sale"
          constraints:
            - type: "positive"
            
        - name: "total_amount"
          type: "decimal"
          description: "Total line amount"
          constraints:
            - type: "positive"

quality:
  rules:
    - name: "amount_consistency"
      description: "Total amount verification"
      severity: "critical"
      check: >
        ABS(total_amount - (quantity * unit_price)) <= 0.01
        
    - name: "referential_integrity"
      description: "Reference verification"
      severity: "critical"
      check: >
        EXISTS(SELECT 1 FROM dim_stores s WHERE s.store_id = store_id) AND
        EXISTS(SELECT 1 FROM dim_products p WHERE p.product_id = product_id)
        
    - name: "deduplication"
      description: "Duplicate detection"
      severity: "warning"
      check: >
        COUNT(*) = COUNT(DISTINCT transaction_id)

processing:
  scheduling:
    frequency: "hourly"
    dependencies:
      - "sales_transactions_bronze"
      - "dim_stores"
      - "dim_products"
  expectations:
    volume:
      min_rows: 1000
      max_rows: 1000000
    latency: "30m"

sla:
  freshness: "1h"
  availability: "99.9%"
  monitoring:
    metrics:
      - name: "quality_score"
        description: "Percentage of rows meeting all rules"
        threshold: 0.99
      - name: "processing_time"
        threshold: "15m"
      - name: "incremental_volume"
        description: "Number of new rows per run"
        alert:
          min: 100
          max: 100000
```

This contract introduces several fundamental concepts adapted to the datalake medallion context:

1. **Multi-level Quality Control**: Quality rules cover both data integrity (transaction uniqueness, amount consistency) and referential integrity with dimensions. This double validation ensures the reliability of downstream analyses.

2. **Batch-Adapted SLAs**: Freshness and availability metrics are calibrated for hourly batch processing, with clear expectations on data volumes expected at each run.

3. **Data Engineering-Oriented Monitoring**: Tracking integrates data processing-specific metrics, such as quality rate and volume variations, essential for detecting anomalies in the processing chain.

## Where to Start?

In a Data Mesh context, the adoption of data contracts must align with the maturity of domains as data producers. I've observed that organizations succeed better when they:

1. Identify a mature and motivated business domain to pilot the initiative. In retail, the sales domain often plays this role, creating a concrete example for other domains.
2. Start with a critical data product having multiple consumers. The silver transactions table is perfect: critical data for reporting, multiple analytical consumers, clear quality needs.
3. Establish a short feedback loop with consumers. Data scientists analyzing purchase behaviors provide valuable feedback on necessary attributes and their quality constraints.
4. Progressively automate validations and monitoring, transforming the contract into a living tool rather than static documentation.
5. Document and share successes to create a snowball effect. When other domains see the reduction in incidents and improvement in analysis reliability, they naturally adopt the approach.

The goal isn't immediate perfection, but to establish a new standard for collaboration around data. If you want the adoption of data contracts to be successful, everyone needs to be involved and respect the format, otherwise your production deployment will fail.

## Conclusion

Data contracts in a datalake aren't just documentation - they become the guardrail that ensures data quality and reliability at each transformation step. By formalizing expectations and responsibilities, they create a trust framework that allows building reliable analyses on quality data.

In the next article, we'll explore how these contracts integrate into a global data governance strategy, emphasizing the evolution and maintenance of contracts over time.

## Reference Implementation

The concepts presented in this article are implemented in the following files:

- [Basic Data Contract](../../../contracts/customer-domain/order_events.yaml) - Simple contract example
- [Advanced Data Contract](../../../contracts/customer-domain/customer_profile_events.yaml) - Contract with quality rules
- [Validation Tests](../../../validation/contract_tests.py) - Validation implementation

To get started with these examples, consult the [quick start guide](../../../README.md#-quick-start).
